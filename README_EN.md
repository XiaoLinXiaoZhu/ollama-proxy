# ollama-proxy

A locally running Ollama-compatible proxy service supporting seamless integration with multiple model providers

## Features

- ü¶ô Partial Ollama API compatibility: Implements core APIs for VSCode Copilot integration
- üõ°Ô∏è Local proxy service: Provides Ollama-like API interface at 127.0.0.1:11434
- üîå Multi-provider support: Integration with Novita/SiliconFlow/Groq/xAI and other major platforms
- üîÑ Dynamic configuration: Reuses Continue.dev configuration standard with YAML hot reload
- üß© Protocol adaptation: Full implementation of Ollama core API specifications
- üîç Debug mode: Detailed request/response logging

## Use Cases

- ‚úÖ VSCode Copilot custom model integration (verified compatibility)

## Implemented APIs

| Endpoint                | Method | Description                  | Compatibility |
|-------------------------|--------|------------------------------|---------------|
| `/v1/models`            | GET    | Get available model list     | ‚úÖ 100%       |
| `/api/tags`             | GET    | Get model tag information    | ‚úÖ 100%       |
| `/api/show`             | POST   | View model details           | ‚úÖ 100%       |
| `/v1/chat/completions`  | POST   | Chat completion (proxy)      | ‚úÖ 100%       |

## Configuration Details

This proxy reuses the [Continue.dev](https://docs.continue.dev/reference/) configuration standard and can directly use existing Continue configurations:

```yaml
models:
  - name: Novita deepseek v3
    provider: novita
    model: deepseek/deepseek-v3-0324
    apiKey: sk_xxxxx
  - name: Inference.net DeepSeek V3
    provider: openai
    apiBase: https://api.inference.net/v1
    model: deepseek/deepseek-v3-0324/fp-8
    apiKey: inference-xxxxx
  - name: Siliconflow DeepSeek-V3
    provider: siliconflow
    model: deepseek-ai/DeepSeek-V3
    apiKey: sk-xxxxxx
```

## Development Guide

Build command:
```bash
go build -o ollama-proxy
```

Start service:
```bash
./ollama-proxy -config /path/to/config.yaml
```



## FAQ

### How to enable debug mode?
Add `-debug` parameter when starting:
```bash
./ollama-proxy -debug
```

### How to apply configuration changes?
The service automatically monitors config file changes and reloads immediately

### How to verify if proxy is working?
```bash
curl http://127.0.0.1:11434/v1/models
```

### How are multi-provider requests routed?
Automatically matched by `model` field in request to `name` in configuration

## Contributing

We welcome contributions from the community!
